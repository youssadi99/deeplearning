{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJZRPwdQtuk8GTHIXI7/+1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercice 1** : Implementation de la porte NAND avec un perceptron"
      ],
      "metadata": {
        "id": "2H90DvXCJnYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "E8uOuIVxJy5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpq4Pwx0JcEr"
      },
      "outputs": [],
      "source": [
        "#TODO :First element in vector x must be 1.\n",
        "# Length of w and x must be n+1 for neuron with n inputs.\n",
        "def compute_output(w, x):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now walk through a Python implementation of this algorithm and apply it to our NAND example."
      ],
      "metadata": {
        "id": "FUZaKzfKKaXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO :Define training examples.\n",
        "x_train =  # Inputs\n",
        "y_train =  # Output (ground truth)"
      ],
      "metadata": {
        "id": "NMZ9TkLPLUlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables needed for plotting.\n",
        "color_list = ['r-', 'm-', 'y-', 'c-', 'b-', 'g-']\n",
        "color_index = 0\n",
        "def show_learning(w):\n",
        "    global color_index\n",
        "    print('w0 =', '%5.2f' % w[0], ', w1 =', '%5.2f' % w[1], ', w2 =', '%5.2f' % w[2])\n",
        "    if color_index == 0:\n",
        "        plt.plot([1.0], [1.0], 'b_', markersize=12)\n",
        "        plt.plot([-1.0, 1.0, -1.0], [1.0, -1.0, -1.0], 'r+', markersize=12)\n",
        "        plt.axis([-2, 2, -2, 2])\n",
        "        plt.xlabel('x1')\n",
        "        plt.ylabel('x2')\n",
        "    x = [-2.0, 2.0]\n",
        "    if abs(w[2]) < 1e-5:\n",
        "        y = [-w[1]/(1e-5)*(-2.0)+(-w[0]/(1e-5)), -w[1]/(1e-5)*(2.0)+(-w[0]/(1e-5))]\n",
        "    else:\n",
        "        y = [-w[1]/w[2]*(-2.0)+(-w[0]/w[2]), -w[1]/w[2]*(2.0)+(-w[0]/w[2])]\n",
        "    plt.plot(x, y, color_list[color_index])\n",
        "    if color_index < (len(color_list) - 1):\n",
        "        color_index += 1"
      ],
      "metadata": {
        "id": "f04q7_lUNbFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables needed to control training process.\n",
        "random.seed(7) # To make repeatable\n",
        "LEARNING_RATE = 0.1\n",
        "index_list = [0, 1, 2, 3] # Used to randomize order"
      ],
      "metadata": {
        "id": "1kUxbgyFLAbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define perceptron weights.\n",
        "w = [0.2, -0.6, 0.25] # Initialize to some \"random\" numbers\n",
        "# Print initial weights.\n",
        "show_learning(w)"
      ],
      "metadata": {
        "id": "yF4GuiL0JmO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO :perceptron algorithm: should invoke showlearning for each iteration\n",
        "def perceptron():\n"
      ],
      "metadata": {
        "id": "cPeQkfmILxIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercice 2** : Implementation de la porte XOR en utilisant un MLP"
      ],
      "metadata": {
        "id": "eCG4TDDqPBVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(3) # To make repeatable\n",
        "LEARNING_RATE = 0.1\n",
        "index_list = [0, 1, 2, 3] # Used to randomize order\n",
        "# Define training examples.\n",
        "x_train =  # Input\n",
        "y_train =  # Output (ground truth)"
      ],
      "metadata": {
        "id": "2ylDB42MPMT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neuron_w(input_count):\n",
        "    weights = np.zeros(input_count+1)\n",
        "    for i in range(1, (input_count+1)):\n",
        "        weights[i] = np.random.uniform(-1.0, 1.0)\n",
        "    return weights"
      ],
      "metadata": {
        "id": "ASNcSQUJP6BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO :\n",
        "n_w =\n",
        "n_y =\n",
        "n_error =\n",
        "print(n_w)"
      ],
      "metadata": {
        "id": "yuDFv4gQQMPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_learning():\n",
        "    print('Current weights:')\n",
        "    for i, w in enumerate(n_w):\n",
        "        print('neuron ', i, ': w0 =', '%5.2f' % w[0],', w1 =', '%5.2f' % w[1], ', w2 =','%5.2f' % w[2])\n",
        "    print('----------------')\n",
        "\n",
        "show_learning()"
      ],
      "metadata": {
        "id": "4bSSxeK4QcmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO :we use tanh for hidden layer and sigmoid for output layer\n",
        "def forward_pass(x):\n",
        "    global n_y\n",
        ""
      ],
      "metadata": {
        "id": "2-FEBobWQ3c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO :backpropagation: using formulas as studied in our lesson (chapter 2)\n",
        "def backward_pass(y_truth):\n",
        "    global n_error"
      ],
      "metadata": {
        "id": "aVYlS2JuRVTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO :Network training loop.\n"
      ],
      "metadata": {
        "id": "Pji1AOnbR97x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercice 3** : Développement d'un réseau feedforward (ANN profond) : un classifieur binaire. A noter d'utiliser en couches cachées la fonction tanh et en couche de sortie la fonction sigmoid."
      ],
      "metadata": {
        "id": "R6dKDWK-TBy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, make_circles\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Bc1pcwPDTnuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "TMbmmyZCUGOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : binary cross entropy\n",
        "def log_loss(A, y):\n",
        "\n"
      ],
      "metadata": {
        "id": "adejF6aqUKrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialisation(dimensions):\n",
        "\n",
        "  parametres= {}\n",
        "  C = len(dimensions)\n",
        "  for c in range(1, C):\n",
        "    parametres[\"W\"+ str(c)] = np.random.randn(dimensions[c], dimensions[c-1])\n",
        "    parametres[\"b\"+ str(c)] = np.random.randn(dimensions[c], 1)\n",
        "\n",
        "  return parametres"
      ],
      "metadata": {
        "id": "BcEk44RJU-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = initialisation([2, 32, 32, 1])\n",
        "for key, val in params.items():\n",
        "  print(key, val.shape)"
      ],
      "metadata": {
        "id": "ru4QDhKtVR4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : the function must return the network activations\n",
        "def forward_propagation(X, parametres):\n",
        "\n",
        "  #si pamaretre contient w1, b1, w2, b2 c'est que le reseau contient deux couches soit 4//2\n",
        "  C = len(parametres)//2\n",
        "  activations = {\"A0\" : X}\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "43ajlq-FV5MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : the function must return the layers gradients\n",
        "def back_propagation(y, activations, parametres):\n",
        "  C = len(parametres)//2\n",
        "  m = y.shape[1]\n",
        "  gradients = {}\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "FxVK2yAdWQCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : the function must return the updated parameters\n",
        "def update(gradients, parametres,  learning_rate):\n",
        "  C = len(parametres)//2"
      ],
      "metadata": {
        "id": "vna4-fAbWmoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, parametres):\n",
        "  activations = forward_propagation(X, parametres)\n",
        "  C = len(parametres)//2\n",
        "  return activations[\"A\"+str(C)] >= 0.5\n"
      ],
      "metadata": {
        "id": "3RKDN4luXCC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : ANN implementation : the function returns the network parameters\n",
        "def neuron_network(X, y, hidden_layers= (32, 32, 32), learning_rate=0.1, n_iter=1000):\n",
        "  np.random.seed(0)\n",
        "  #initialisation\n",
        "  dimensions = list(hidden_layers)\n",
        "  dimensions.insert(0, X.shape[0])\n",
        "  dimensions.append(y.shape[0])\n",
        "\n",
        "  parametres = initialisation(dimensions)\n",
        "\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "\n",
        "  for i in tqdm(range(n_iter)):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(14, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_loss, label='train loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_acc, label='train accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return parametres"
      ],
      "metadata": {
        "id": "AsIiMPthXs0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=0)\n",
        "X = X.T\n",
        "y = y.reshape(1, y.shape[0])\n",
        "print(\"sahpex\", X.shape)\n",
        "print(\"sahpey\", y.shape)\n",
        "params = neuron_network(X, y, learning_rate=0.1, n_iter=1000)"
      ],
      "metadata": {
        "id": "6hoRKbsxYQwP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}